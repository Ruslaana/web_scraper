{
    "version": "1.0",
    "document": {
        "id": 10782,
        "title": "Orwell, Kafka og kunstig intelligens i morgendagens Danmark",
        "content": "Kronikørerne er henholdsvis lektor og professor ved Institut for Fødevare- og Ressourceøkonomi på Københavns Universitet, hvor de netop i samarbejde med universitetets datologer har påbegyndt et forskningsprojekt om etik og brugen af kunstig intelligens. Danmark skal ifølge regeringen være et databrugsland. Ambitionen er, atvi i 2025 skal være blandt de bedste i verden til at udnytte data både i erhvervslivet og i det offentlige, ved hjælp af kunstig intelligens. Men hvor går de etiske grænser i et samfund, hvor indsamling, opbevaring, behandling og udveksling af data om stort set alle aspekter af borgernes liv allerede foregår i enorm skala? Sune HolmFold sammenLæs mere Et vigtigt område, hvor der er behov for etisk agtpågivenhed, er brugen af såkaldte forudsigelsesalgoritmer i den offentlige sektor. Disse vil på den ene side kunne tjene til at forebygge alvorlige menneskelige problemer, men på den anden side vil de kunne give anledning til overskridelse af vigtige etiske grænser. Mange kan stadig huske »Tøndersagen«. I Tønder og Løgumkloster kommune blev to børn i en årrække udsat for seksuelt misbrug. Efterfølgende var et kritikpunkt, at myndighederne ikke greb ind meget tidligere, end de gjorde. Baggrunden for den ulykkelige sag er kompleks, men et af problemerne var, at de forskellige myndigheder ikke talte sammen og ikke delte deres oplysninger. I skrivende stund er debatten atter aktuel i forbindelse medretssagen mod et forældrepar fra Vejen, der siden 2017 er fængslet og tiltalt for syv års »særdeles groft misbrug« af deres fire børn. Måske sager som Tønder- og Vejen-sagerne kunne være undgået, hvis myndighederne havde delt deres data med et computerprogram, der kunne udpege børn med særlig høj risiko for mistrivsel? I Gladsaxe Kommune har man arbejdet på at træneen algoritmei at identificere udsatte børn på basis af samkøring af blandt andet beskæftigelsesdata, data fra tandpleje, sundhedspleje, flyttehistorik og rusmiddelbehandling. Tanken er, at computersystemet skal hjælpe kommunen med at målrette sin opsøgende og forebyggende indsats i forhold til udsatte børn. Andre forudsigelsessystemer er allerede på trapperne til brug i det offentlige. Fx samarbejder flere regioner allerede nu med private aktører om at udvikle algoritmer, der kan forudsige, hvilke borgere der vil blive akut indlagt i den nærmeste fremtid. En udbredt kritik af Gladsaxe Kommunes initiativ, som netop er sat på pause, er, at det udgør et skridt på vejen mod et Big Brother samfund, som i George Orwells dystopiske roman1984. Her beskrives et samfund, hvor staten overvåger alt, hvad borgerne gør.  Vi mener ikke, at Big Brother metaforen er fuldt dækkende for de problemstillinger, der er forbundet med Gladsaxemodellen. Man anvender her data om forhold, som for de enkelte borgere isoleret set ikke er særligt prekære. Problemet opstår fordi oplysninger, som borgerne deler med offentlige institutioner, bliver (gen)anvendt i computerdrevet dataanalyse til at finde ud afandreting om borgerne. Den amerikanske jurist Daniel Solove har foreslået en anden metafor for den etiske problemstilling, som her er på spil: Franz KafkasProcessen. I Kafkas historie får hovedpersonen Josef K. en dag at vide, at han er arresteret. I resten af romanen følger vi Josef K.s forgæves kamp for at finde ud af, hvad han er arresteret for, og hvem der står bag.Processenbeskriver således et individs magtesløshed over for et uigennemskueligt bureaukrati, der anvender oplysninger om folk til at tage vigtige beslutninger om dem, uden at de berørte har viden om og mulighed for at deltage i processen. Peter SandøeFold sammenLæs mere Vi mener, at Kafka og Orwells fortællinger komplementerer hinanden som metaforer for det etiske problemkompleks, der kendetegner eksempelvis Gladsaxemodellen og anvendelsen af forudsigelsesalgoritmer i det offentlige mere generelt. Særligt tre etiske værdier bliver udfordret : Gennemsigtighed, ansvar og integritet. Hvorfor opsøgte I lige vores familie? Hvad har vi gjort forkert? Hvis de systemer, der ligger til grund for udpegningen af et konkret individ, ikke er gennemsigtige for hverken brugeren eller borgeren, bliver det svært at give tilfredsstillende svar på disse spørgsmål. Manglende gennemsigtighed kan skyldes flere ting. Forudsigelsesalgoritmen kan være så kompleks, at selv ikke eksperter kan forstå og forklare, hvordan den når frem til sine anbefalinger. En anden mulighed er, at den er en forretningshemmelighed. Når det kommer til anvendelse af forudsigelsesalgoritmer i det offentlige, mener vi, at disse som minimum bør være genstand for uafhængig kontrol. Desuden må baggrunden for deres anbefalinger i store træk kunne forklares af de medarbejdere, der anvender dem som beslutningsstøtte. Ellers har vi opskriften på et kafkask scenario. I forhold til værdien ansvar er det vigtigt at kunne slå fast, hvem der tager beslutninger baseret på en algoritmes anbefalinger. Selv hvis vi fastholder, at algoritmerne kun skal anvendes som beslutningsstøtte, vil denne støtte for de fleste beslutningstageres vedkommende være som at konsultere et orakel, hvis brugerne af algoritmen ikke aner, hvordan den kommer frem til sine anbefalinger. Hvis en medarbejder oven i købet ved, at algoritmen har en højere succesrate end mennesker, hvor efterlader det så medarbejderen? Vi mener, at det kan være meget svært for en medarbejder at afvise en anbefaling fra en algoritme i sådanne situationer. Risikoen er, at vi under betegnelsen »beslutningsstøtte«de factofår en automatiseret proces. Med hensyn til værdien integritet er det i Danmark en bredt accepteret norm, at vigtige beslutninger om borgerne bør tages på baggrund af en konkret og individuel vurdering. Det kan en algoritme ikke. Den ser alene på, om en person statistisk set tilhører en bestemt risikogruppe. For så vidt der ikke er tale om fuldt automatiserede beslutningsgange, men derimod om anvendelse af forudsigelsesalgoritmer som beslutningsstøtte, vil der i princippet være rum for en helhedsvurdering. En menneskelig beslutningstager kan skønne, at algoritmens anbefaling ikke bør følges og se bort fra den på baggrund af omstændigheder, som algoritmen ikke kan tage højde for. Men som allerede nævnt, er der store udfordringer forbundet med at lade det være op til den enkelte medarbejder at afvise algoritmers anbefalinger. En udbredt kritik af Gladsaxe Kommunes initiativ, som netop er sat på pause, er, at det udgør et skridt på vejen mod et Big Brother samfund, som i George Orwells dystopiske roman 1984. Her beskrives et samfund, hvor staten overvåger alt, hvad borgerne gør.&nbsp;&nbsp;Fold sammenLæs mereFoto: Pawel Kopczynski/Scanpix 2017. Tønder- og Vejen-sagerne er skrækeksempler på, hvor galt det kan gå, når myndighederne ikke griber ind i tide. Skal vi introducere forudsigelsesalgoritmer for at sikre os bedre mod sager som disse? Som vi har beskrevet, er der en række etiske udfordringer forbundet med modeller som den, Gladsaxe kommune har forsøgt at udvikle. Men det er heller ikke uden omkostninger at undlade at introducere forudsigelsesalgoritmer i det offentlige, hvis disse systemer faktisk kan sætte os i stand til bedre at forebygge og mindske tilfælde af mistrivsel blandt børn. For os at se bør disse omkostninger tages med i overvejelsen. Vi mener dog, at der er grund til at være varsom med at presse anvendelsen af forudsigelsesalgoritmer igennem i den offentlige forvaltning. At sikre at anvendelsen af sådanne algoritmer sker på forsvarlig vis, kan tage tid og som udgangspunkt kræve ekstra investeringer (og ikke sikre hurtige besparelser). Det drejer sig om investeringer i uddannelse af medarbejdere, i udvikling af gennemskuelige og forklarlige algoritmer og i information til og dialog med borgerne om, hvordan deres data bliver anvendt til at udvikle nye systemer, og hvad de nye systemer bidrager med. Selv med de bedste intentioner og forberedelser er der altid en risiko for, at noget går galt. Uden en åben og nuanceret debat i samfundet om de gevinster og omkostninger, der følger med brugen af forudsigelsesalgoritmer, kan man frygte, at kafkaske scenarier vil føre til et bagslag for udviklingen af en teknologi, som anvendt med rettidig omhu kan bidrage til at forebygge alvorlige menneskelige problemer.",
        "metadata": {
            "source": "https://www.berlingske.dk/kronikker/orwell-kafka-og-kunstig-intelligens-i-morgendagens-danmark",
            "image_url": "https://berlingske.bmcdn.dk/media/cache/resolve/image_x_large_vertical/image/111/1118254/22655422-.jpg",
            "publication_time": "Tirsdag d. 05. marts 2019, kl. 18.02",
            "author": null,
            "related_links": [],
            "attachments": []
        }
    }
}